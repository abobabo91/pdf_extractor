


import streamlit as st
st.set_page_config(layout="wide")
import pandas as pd
import numpy as np
import os
import PyPDF2
from pdf2image import convert_from_bytes
import gc
import pytesseract
from io import BytesIO
import openai
from openai import OpenAI
import tiktoken
import re

import traceback, sys
import gc

def global_exception_handler(exc_type, exc_value, exc_traceback):
    st.error("Unhandled exception:")
    st.code("".join(traceback.format_exception(exc_type, exc_value, exc_traceback)))

sys.excepthook = global_exception_handler


MODEL_PRICES = {
    "gpt-4o": {"input": 2.50, "output": 10.00},
    "gpt-4.1": {"input": 1.25, "output": 10.00},
    "gpt-4.1-mini": {"input": 0.25, "output": 2.00},
    "gpt-4.1-nano": {"input": 0.05, "output": 0.40},
}


def df_ready(df, required_cols=None):
    """Ellen≈ërzi, hogy a df nem √ºres √©s (opcion√°lisan) tartalmazza a k√∂telez≈ë oszlopokat."""
    if not isinstance(df, pd.DataFrame) or df is None or df.empty:
        return False
    if required_cols:
        return all(col in df.columns for col in required_cols)
    return True

def need_msg(missing_list):
    bullets = "\n".join([f"- {m}" for m in missing_list])
    st.warning(f"Az √∂sszef≈±z√©shez a k√∂vetkez≈ëk hi√°nyoznak vagy √ºresek:\n{bullets}")

def count_tokens(text, model="gpt-4o"):
    encoder = tiktoken.encoding_for_model(model)
    tokens = encoder.encode(text)
    return len(tokens)

def replace_successive_duplicates(df, column_to_compare, columns_to_delete):
    result = df.copy()
    col = column_to_compare
    mask = result[col] == result[col].shift()
    for col in columns_to_delete:
        result.loc[mask, col] = np.nan
    return result



def extract_text_from_pdf(uploaded_file):
    file_name = uploaded_file.name
    pdf_content = ""

    # 1) sima sz√∂vegkinyer√©s
    try:
        pdf_reader = PyPDF2.PdfReader(uploaded_file)
        for page in pdf_reader.pages:
            pdf_content += page.extract_text() or ""
    except Exception as e:
        st.error(f"Hiba a(z) {file_name} f√°jl olvas√°sakor: {e}")
        return None

    # 2) OCR fallback, ha t√∫l kev√©s sz√∂veg van
    if len(pdf_content.strip()) < 100:
        pdf_content = ""
        try:
            uploaded_file.seek(0)
            file_bytes = uploaded_file.read()

            # el≈ësz√∂r der√≠ts√ºk ki h√°ny oldal van
            num_pages = len(PyPDF2.PdfReader(BytesIO(file_bytes)).pages)

            progress = st.progress(0)
            for i in range(1, num_pages + 1):
                images = convert_from_bytes(file_bytes, dpi=150, first_page=i, last_page=i)
#                images = convert_from_bytes(file_bytes, dpi=150, first_page=i, last_page=i, poppler_path = r"C:\poppler-24.08.0\Library\bin") #local)
                text = pytesseract.image_to_string(images[0], lang="hun")
                pdf_content += text + "\n"

                # mem√≥riatiszt√≠t√°s
                del images
                gc.collect()

                progress.progress(i / num_pages)

        except Exception as e:
            st.error(f"OCR hiba a(z) {file_name} f√°jln√°l: {e}")
            return None

    # 3) hosszkorl√°toz√°s
    if len(pdf_content) > 300000:
        st.warning(file_name + " t√∫l hossz√∫, csak az els≈ë 300000 karakter ker√ºl feldolgoz√°sra.")
        pdf_content = pdf_content[:300000]

    return pdf_content




def generate_gpt_prompt(text):
    """Generates a clear, structured GPT prompt for invoice data extraction."""
    return (
        "You are given the extracted text of a Hungarian invoice PDF. "
        "The PDF may contain multiple invoices merged together. "
        "Your task is to extract the following **9 data fields** for each invoice:\n\n"
        "1. Seller name (string)\n"
        "2. Buyer name (string)\n"
        "3. Invoice number (string)\n"
        "4. Invoice date (string, e.g. '2024.04.01')\n"
        "5. Total gross amount (integer)\n"
        "6. Total net amount (integer)\n"
        "7. VAT amount (integer)\n"
        "8. Currency (string: 'HUF' or 'EUR')\n"
        "9. Exchange rate (integer, use 1 if invoice is in HUF)\n\n"
        "**Important formatting instructions:**\n"
        "- Use semicolon (`;`) to separate the 9 fields.\n"
        "- Use **one line per invoice**.\n"
        "- Do **not** include field numbers (e.g. '1)', '2)' etc.) in the output.\n"
        "- Write all numeric fields as plain integers (e.g. `1500000`).\n"
        "- **Do not use thousands separators** (e.g. `.`) or decimal commas (`,`) in the output and only output the integer part of the numbers.\n"
        "- Note: In Hungarian, decimal separators are commas (`,`) instead of dots (`.`) and thousand separators are dots (`.`)"
        "- Do **not** include any explanation, headings, or extra text ‚Äî just the data rows.\n\n"
        "Extracted text:\n"
        f"{text}"
    )


def extract_data_with_gpt(file_name, text, model_name):
    """A kiv√°lasztott GPT modellel kinyeri a strukt√∫r√°lt adatokat a PDF sz√∂vegb≈ël."""
    gpt_prompt = generate_gpt_prompt(text)

    try:
        client = OpenAI(api_key=openai.api_key)
        response = client.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "system", "content": ""},
                {"role": "user", "content": gpt_prompt}],
            max_completion_tokens=5000,
            timeout=30
        )

        raw_output = response.choices[0].message.content.strip()
        rows = raw_output.split("\n")

        parsed_rows = []
        for row in rows:
            parts = row.strip().split(";")
            if len(parts) != 9:
                st.warning(f"Hib√°s sor ({file_name}): {row}")
                continue

            cleaned_parts = [re.sub(r"^\s*\d+\)\s*", "", p.strip()) for p in parts]
            parsed_rows.append([file_name] + cleaned_parts)

        return parsed_rows, count_tokens(gpt_prompt)

    except Exception as e:
        st.error(f"A {model_name} feldolgoz√°s sikertelen volt: {file_name} ‚Äì {e}")
        return [], 0



def normalize_number(value):
    """Converts numeric-looking strings or floats to int. Removes all formatting."""
    try:
        if pd.isna(value):
            return None
        if isinstance(value, (int, float)):
            return int(round(value))
        # Remove thousands separators ('.' or ',' or space), allow decimals
        cleaned = str(value).replace(" ", "").replace(",", "").replace(".", "")
        return int(cleaned)
    except:
        return None


def compare_with_tolerance(val1, val2, tolerance=500):
    try:
        val1 = normalize_number(val1)
        val2 = normalize_number(val2)

        # Ha b√°rmelyik hi√°nyzik ‚Üí "No Data"
        if val1 is None or val2 is None or pd.isna(val1) or pd.isna(val2):
            return "Nincs adat"

        return "Igen" if abs(val1 - val2) <= tolerance else "Nem"
    except Exception:
        return "Nincs adat"

    
    

def get_minta_amount(row, huf_col="√ârt√©k", eur_col="√ârt√©k deviza", currency_col="Devizanem"):
    """Returns the value in correct currency column based on Devizanem."""
    try:
        dev = str(row[currency_col]).strip().upper()
        if dev == "EUR":
            return normalize_number(row[eur_col])
        else:
            return normalize_number(row[huf_col])
    except:
        return None




def merge_with_minta(df_extracted, df_minta, invoice_col_extracted="Sz√°mlasz√°m", invoice_col_minta="Bizonylatsz√°m"):
    df_merged = pd.merge(df_minta, df_extracted, how='outer', left_on=invoice_col_minta, right_on=invoice_col_extracted)
    matched = df_merged[invoice_col_extracted].notna().sum()
    total = len(df_minta)
    unmatched = total - matched
    match_rate = round(100 * matched / total, 2)
    
    stats = {
        "√ñsszes minta sor": total,
        "Tal√°latok sz√°ma": matched,
        "Hi√°nyz√≥ tal√°latok": unmatched,
        "Egyez√©si ar√°ny (%)": match_rate
    }

    return df_merged, stats



# Inicializ√°ljuk a session state v√°ltoz√≥kat
if 'extracted_text_from_invoice' not in st.session_state:
    st.session_state.extracted_text_from_invoice = []
if 'extracted_data' not in st.session_state:
    st.session_state.extracted_data = []
if 'df_extracted' not in st.session_state:
    st.session_state.df_extracted = pd.DataFrame()
if 'df_minta' not in st.session_state:
    st.session_state.df_minta = pd.DataFrame()
if 'df_nav' not in st.session_state:
    st.session_state.df_nav = pd.DataFrame()
if 'df_karton' not in st.session_state:
    st.session_state.df_karton = pd.DataFrame()
if 'df_merged' not in st.session_state:
    st.session_state.df_merged = pd.DataFrame()
if 'df_merged_full' not in st.session_state:
    st.session_state.df_merged_full = pd.DataFrame()
if 'number_of_tokens' not in st.session_state:
    st.session_state.number_of_tokens = 0

openai.organization = "org-i7aicv7Qc0PO4hkTCT4N2BqR"
openai.api_key = st.secrets['openai']["OPENAI_API_KEY"]

st.title("üìÑ Sz√°mlaadat-kinyer≈ë alkalmaz√°s")

col_pdf, col_excel = st.columns([1, 1])  # nagyobb bal oldali has√°b

with col_pdf:
    st.subheader("üìÇ PDF-ek kinyer√©se")
    
    st.write("1) T√∂lts fel egy vagy t√∂bb **magyar nyelv≈± sz√°ml√°t (PDF)**, amelyekb≈ël a rendszer kiolvassa a legfontosabb adatokat.")
    
    # 0) F√°jl felt√∂lt≈ë
    uploaded_files = st.file_uploader("üì§ PDF f√°jlok felt√∂lt√©se", type=["pdf"], accept_multiple_files=True)
    
    selected_model = st.selectbox(
        "V√°lassz modellt az adatkinyer√©shez:",
        ["gpt-4o", "gpt-4.1", "gpt-4.1-mini", "gpt-4.1-nano"],
        index=0,
        help="√Årak per 1M token:\n"
             "- gpt-4o: \\$2.50 input / \\$10 output\n"
             "- gpt-4.1: \\$1.25 input / \\$10 output\n"
             "- gpt-4.1-mini: \\$0.25 input / \\$2 output\n"
             "- gpt-4.1-nano: \\$0.05 input / \\$0.40 output"
        )
    
    # 1) PDF feldolgoz√°s
    if st.button("üìë Adatkinyer√©s a PDF-b≈ël"):  
        st.session_state.extracted_text_from_invoice = []      
        if uploaded_files:
            if len(uploaded_files) > 200:
                st.write("‚ö†Ô∏è Az els≈ë 200 f√°jl ker√ºl feldolgoz√°sra.")
            files_to_process = uploaded_files[:200]
    
            # ---- 1) PDF sz√∂veg kinyer√©s progress bar ----
            pdf_progress = st.progress(0, text="PDF sz√∂veg kinyer√©se folyamatban...")
            pdf_status = st.empty()  # helyfoglal√≥ a st√°tusznak
    
            for idx, uploaded_file in enumerate(files_to_process, start=1):
                file_name = uploaded_file.name
                pdf_status.write(f"{file_name} feldolgoz√°sa (PDF sz√∂veg kinyer√©se)...")
                pdf_text = extract_text_from_pdf(uploaded_file)
    
                if pdf_text is None:
                    continue
    
                st.session_state.extracted_text_from_invoice.append([file_name, pdf_text])
    
                # update progress
                pdf_progress.progress(idx / len(files_to_process), 
                                      text=f"PDF kinyer√©s: {idx}/{len(files_to_process)} k√©sz")
    
            pdf_progress.empty()
            pdf_status.write("‚úÖ PDF sz√∂veg kinyer√©s befejezve.")
    
        else:
            st.warning("‚ö†Ô∏è K√©rlek, t√∂lts fel legal√°bb egy PDF f√°jlt.")
    
        # ---- 2) GPT adatkinyer√©s progress bar ----
        st.session_state.extracted_data = []
        if st.session_state.extracted_text_from_invoice:
            gpt_progress = st.progress(0, text="AI adatkinyer√©s folyamatban...")
            gpt_status = st.empty()  # helyfoglal√≥ a st√°tusznak
    
            for idx, (file_name, pdf_content) in enumerate(st.session_state.extracted_text_from_invoice, start=1):
                gpt_status.write(f"{file_name} feldolgoz√°sa (AI adatkinyer√©s)...")
                extracted_rows, tokens = extract_data_with_gpt(file_name, pdf_content, selected_model)
                if extracted_rows:
                    st.session_state.extracted_data.extend(extracted_rows)
                    st.session_state.number_of_tokens += tokens
    
                # update progress
                gpt_progress.progress(idx / len(st.session_state.extracted_text_from_invoice),
                                      text=f"AI feldolgoz√°s: {idx}/{len(st.session_state.extracted_text_from_invoice)} k√©sz")
    
            gpt_progress.empty()
            gpt_status.write("‚úÖ AI adatkinyer√©s befejezve.")
    
        # ---- 3) DataFrame l√©trehoz√°s ----
        if st.session_state.extracted_data:
            st.session_state.df_extracted = pd.DataFrame(
                st.session_state.extracted_data,
                columns=["F√°jln√©v", "Elad√≥", "Vev≈ë", "Sz√°mlasz√°m", "Sz√°mla kelte", 
                         "Brutt√≥ √°r", "Nett√≥ √°r", "√ÅFA", "Deviza", "√Årfolyam"]
            )
            st.session_state.df_extracted["Sz√°mlasz√°m"] = st.session_state.df_extracted["Sz√°mlasz√°m"].astype(str)
            st.session_state.df_extracted["1"] = np.nan


    
    if len(st.session_state.df_extracted) > 0:        
        st.write("‚úÖ **Adatok kinyerve!** Az al√°bbi t√°bl√°zat tartalmazza az eredm√©nyeket:")
        st.dataframe(st.session_state.df_extracted)
    
#        extract_csv = st.session_state.df_extracted.to_csv(index=False).encode("utf-8")
#        st.download_button("üì• Kinyert adatok let√∂lt√©se CSV-ben", extract_csv, "kinyert_adatok.csv", "text/csv", key="letoltes-csv")
        
        buffer = BytesIO()
        with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
            st.session_state.df_extracted.to_excel(writer, sheet_name='Adatok', index=False)

            st.download_button(
                label="üì• Kinyert adatok let√∂lt√©se Excelben",
                data=buffer,
                file_name='kinyert_adatok.xlsx',
                mime='application/vnd.ms-excel'
            )


    # Token √°r becsl√©s
    price_input = st.session_state.number_of_tokens * MODEL_PRICES[selected_model]["input"] / 1_000_000
    st.write(f"üí∞ A becs√ºlt feldolgoz√°si k√∂lts√©g eddig: **${price_input:.2f}** ({selected_model})")



with col_excel:
    
    st.subheader("üìÇ Excel f√°jlok bet√∂lt√©se")
    
    
    # --- Excel f√°jlok bet√∂lt√©se ---
    st.markdown("1) T√∂ltsd fel a **Mintav√©tel** Excel f√°jlt:")
    uploaded_excel_file_minta = st.file_uploader(
        "üì§ Mintav√©tel Excel felt√∂lt√©se",
        type=["xlsx"],
        accept_multiple_files=False,
        help="Az adatok az els≈ë munkalapon a 10. sort√≥l induljanak, √©s legyen 'Bizonylatsz√°m' nev≈± oszlop."
    )
    
    
    if uploaded_excel_file_minta:
        try:
            st.session_state.df_minta = pd.read_excel(uploaded_excel_file_minta, skiprows=range(1, 9))
            st.session_state.df_minta.columns = list(st.session_state.df_minta.iloc[0])
            st.session_state.df_minta = st.session_state.df_minta.iloc[1:]
            st.session_state.df_minta["Bizonylatsz√°m"] = st.session_state.df_minta["Bizonylatsz√°m"].astype(str)
        except:
            st.warning("‚ùå Nem siker√ºlt beolvasni a Mintav√©tel f√°jlt.")
    
    if len(st.session_state.df_minta) > 0:        
        st.write("‚úÖ **Mintav√©tel bet√∂ltve!** Els≈ë n√©h√°ny sor:")
        st.dataframe(st.session_state.df_minta.head(5))
    
    
    
    # NAV f√°jl
    st.markdown("2) T√∂ltsd fel a **NAV** Excel f√°jlt:")
    uploaded_excel_file_nav = st.file_uploader(
        "üì§ NAV Excel felt√∂lt√©se",
        type=["xlsx"],
        accept_multiple_files=False,
        help="Az adatok az els≈ë munkalapon a 6. sort√≥l induljanak, √©s legyen 'sz√°mlasorsz√°m' nev≈± oszlop."
    )    
    if uploaded_excel_file_nav:
        try:
            st.session_state.df_nav = pd.read_excel(uploaded_excel_file_nav, skiprows=5)
            st.session_state.df_nav["sz√°mlasorsz√°m"] = st.session_state.df_nav["sz√°mlasorsz√°m"].astype(str)
        except:
            st.warning("‚ùå Nem siker√ºlt beolvasni a NAV f√°jlt.")
    
    if len(st.session_state.df_nav) > 0:        
        st.write("‚úÖ **NAV f√°jl bet√∂ltve!** Els≈ë n√©h√°ny sor:")
        st.dataframe(st.session_state.df_nav.head(5))
    
    # Karton f√°jl
    st.markdown("3) T√∂ltsd fel a **Karton** Excel f√°jlt:")
    uploaded_excel_file_karton = st.file_uploader(
        "üì§ Karton Excel felt√∂lt√©se",
        type=["xlsx", "xls"],
        accept_multiple_files=False,
        help="Az adatok az els≈ë munkalapon az A1 cell√°t√≥l induljanak."
    )    
    
    if uploaded_excel_file_karton:
        try:
            # Simply read the Excel file, no special column assumptions
            st.session_state.df_karton = pd.read_excel(uploaded_excel_file_karton)
    
            st.write("‚úÖ **Karton bet√∂ltve!** Els≈ë n√©h√°ny sor:")
            st.dataframe(st.session_state.df_karton.head(5))
    
        except Exception as e:
            st.warning(f"‚ùå Nem siker√ºlt beolvasni a Karton f√°jlt: {e}")
    
     
st.title("üìÑ Ellen≈ërz√©sek")

col_left, col_right = st.columns([1, 1])  # nagyobb bal oldali has√°b

with col_left:
    st.subheader("üìé Kinyert adatok √∂sszef≈±z√©se √©s ellen≈ërz√©se: Mintav√©tel")
    
    invoice_colname_minta = "Bizonylatsz√°m"
        
    if st.button("üîó √ñsszef≈±z√©s √©s ellen≈ërz√©s a Mintav√©tel excellel"):
        # El≈ëfelt√©telek
        missing = []
        if not df_ready(st.session_state.df_extracted, required_cols=["Sz√°mlasz√°m", "Nett√≥ √°r"]):
            missing.append("Kinyert adatok (PDF feldolgoz√°s)")
        if not df_ready(st.session_state.df_minta, required_cols=["Bizonylatsz√°m", "√ârt√©k", "√ârt√©k deviza", "Devizanem"]):
            missing.append("Mintav√©tel Excel")
    
        if missing:
            need_msg(missing)
        else:
            try:
                df_minta = st.session_state.df_minta.copy()
                # Fejl√©cek biztons√°gos tiszt√≠t√°sa (nem haszn√°l .str-t)
                df_minta.columns = [str(c).strip() for c in df_minta.columns]
                df_minta["Bizonylatsz√°m"] = df_minta["Bizonylatsz√°m"].astype(str)
    
                df_gpt = st.session_state.df_extracted.copy()
                df_gpt["Sz√°mlasz√°m"] = df_gpt["Sz√°mlasz√°m"].astype(str)
                
                try:
                    df_gpt.drop('1', axis=1, inplace=True)
                except:
                    pass
    
                # --- √°tnevez√©s suffix-szel ---
                df_gpt = df_gpt.add_suffix("_ai")
                df_minta = df_minta.add_suffix("_minta")
                
                # ‚¨ÖÔ∏è Minta balra, GPT jobbra
                df_merged_minta = pd.merge(
                    df_minta,
                    df_gpt,
                    how="left",
                    left_on="Bizonylatsz√°m_minta",
                    right_on="Sz√°mlasz√°m_ai",
                    
                )
                
                # Nett√≥ √∂sszehasonl√≠t√°s a mint√°val
                df_merged_minta["Nett√≥ egyezik?"] = df_merged_minta.apply(
                    lambda row: compare_with_tolerance(
                        get_minta_amount(
                            row,
                            huf_col="√ârt√©k_minta",
                            eur_col="√ârt√©k deviza_minta",
                            currency_col="Devizanem_minta"
                        ),
                        normalize_number(row.get("Nett√≥ √°r_ai")),
                        tolerance=5
                    ),
                    axis=1
                )
                
                # Minden egyezik? oszlop
                df_merged_minta["Minden egyezik?"] = df_merged_minta["Nett√≥ egyezik?"].map({
                    "Igen": "‚úÖ Igen",
                    "Nem": "‚ùå Nem",
                    "Nincs adat": pd.NA   # vagy np.nan, ha ink√°bb float NaN-et szeretn√©l
                })

                
                df_merged_minta = df_merged_minta.sort_values(
                    by="Minden egyezik?", 
                    ascending=False, 
                    key=lambda col: col.eq("‚úÖ Igen"), 
                    kind="stable"
                ).reset_index(drop=True)

                st.session_state.df_merged_minta = df_merged_minta
    
                # üìä Statisztika
                total = len(df_merged_minta)
                matched = (df_merged_minta["Minden egyezik?"] == "‚úÖ Igen").sum()
                match_rate = round(100 * matched / total, 2)
    
                st.session_state.stats_minta = {
                    "√ñsszes sz√°mla": total,
                    "Minden egyez√©s": matched,
                    "Egyez√©si ar√°ny (%)": match_rate
                }
    
                st.success("‚úÖ √ñsszef≈±z√©s √©s ellen≈ërz√©s a Mintav√©tellel k√©sz!")
    
            except Exception as e:
                st.error(f"V√°ratlan hiba t√∂rt√©nt a Mintav√©tel √∂sszef≈±z√©s sor√°n: {e}")

    
    if "df_merged_minta" in st.session_state:
        st.write("üìÑ **√ñsszef≈±z√∂tt √©s ellen≈ërz√∂tt t√°bl√°zat ‚Äì Mintav√©tel:**")
        st.dataframe(st.session_state.df_merged_minta)
    
        csv_minta = st.session_state.df_merged_minta.to_csv(index=False).encode("utf-8")
    
        buffer = BytesIO()
        with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
            st.session_state.df_merged_minta.to_excel(writer, sheet_name='Minta', index=False)
            
            st.download_button(
                label="üì• Let√∂lt√©s Excel (Mintav√©tel)",
                data=buffer,
                file_name='merged_minta.xlsx',
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
    
        st.markdown("### üìä Statisztika ‚Äì Mintav√©tel ellen≈ërz√©s")
        for k, v in st.session_state.stats_minta.items():
            st.write(f"**{k}:** {v}")



with col_right:
    st.subheader("üìé Kinyert adatok √∂sszef≈±z√©se √©s ellen≈ërz√©se: NAV")
    
    if st.button("üîó √ñsszef≈±z√©s √©s ellen≈ërz√©s a NAV excellel"):
        # El≈ëfelt√©telek
        missing = []
        if not df_ready(st.session_state.df_extracted, required_cols=["Sz√°mlasz√°m", "Brutt√≥ √°r", "Nett√≥ √°r", "√ÅFA"]):
            missing.append("Kinyert adatok (PDF feldolgoz√°s)")
        nav_required = ["sz√°mlasorsz√°m"]
        if not df_ready(st.session_state.df_nav, required_cols=nav_required):
            missing.append("NAV Excel")
    
        if missing:
            need_msg(missing)
        else:
            try:
                # NAV adat el≈ëk√©sz√≠t√©s
                df_nav = st.session_state.df_nav.copy()
                df_nav.columns = [str(c).strip() for c in df_nav.columns]
                df_nav["sz√°mlasorsz√°m"] = df_nav["sz√°mlasorsz√°m"].astype(str)
                
                # GPT adat el≈ëk√©sz√≠t√©s
                df_gpt = st.session_state.df_extracted.copy()
                df_gpt["Sz√°mlasz√°m"] = df_gpt["Sz√°mlasz√°m"].astype(str)
                
                # --- √°tnevez√©s suffix-szel ---
                df_gpt = df_gpt.add_suffix("_ai")
                df_nav = df_nav.add_suffix("_nav")
                
                # NAV aggreg√°l√°s sz√°mlasz√°m szinten
                agg_dict = {}
                for col in ["brutt√≥ √©rt√©k_nav", "brutt√≥ √©rt√©k Ft_nav", "nett√≥√©rt√©k_nav", "nett√≥√©rt√©k Ft_nav", "ad√≥√©rt√©k_nav", "ad√≥√©rt√©k Ft_nav"]:
                    if col in df_nav.columns:
                        agg_dict[col] = "sum"
                df_nav_sum = df_nav.groupby("sz√°mlasorsz√°m_nav", as_index=False).agg(agg_dict)
                
                # √ñsszef≈±z√©s sz√°mlasz√°m szint≈± √∂sszehasonl√≠t√°shoz
                df_check = pd.merge(
                    df_gpt,
                    df_nav_sum,
                    how="left",
                    left_on="Sz√°mlasz√°m_ai",
                    right_on="sz√°mlasorsz√°m_nav"
                )
                
                # Oszlopnevek rugalmas keres√©se
                brutto_col = next((c for c in ["brutt√≥ √©rt√©k_nav", "brutt√≥ √©rt√©k Ft_nav"] if c in df_check.columns), None)
                netto_col  = next((c for c in ["nett√≥√©rt√©k_nav", "nett√≥√©rt√©k Ft_nav"] if c in df_check.columns), None)
                afa_col    = next((c for c in ["ad√≥√©rt√©k_nav", "ad√≥√©rt√©k Ft_nav"] if c in df_check.columns), None)
                
                # √ñsszegellen≈ërz√©sek
                df_check["Brutt√≥ egyezik?"] = df_check.apply(
                    lambda row: compare_with_tolerance(
                        normalize_number(row.get(brutto_col)) if brutto_col else None,
                        normalize_number(row.get("Brutt√≥ √°r_ai")),
                    ),
                    axis=1
                )
                df_check["Nett√≥ egyezik?"] = df_check.apply(
                    lambda row: compare_with_tolerance(
                        normalize_number(row.get(netto_col)) if netto_col else None,
                        normalize_number(row.get("Nett√≥ √°r_ai")),
                    ),
                    axis=1
                )
                df_check["√ÅFA egyezik?"] = df_check.apply(
                    lambda row: compare_with_tolerance(
                        normalize_number(row.get(afa_col)) if afa_col else None,
                        normalize_number(row.get("√ÅFA_ai")),
                    ),
                    axis=1
                )
                df_check["Minden egyezik?"] = df_check.apply(
                    lambda row: "‚úÖ Igen" if (row["Brutt√≥ egyezik?"] and row["Nett√≥ egyezik?"] and row["√ÅFA egyezik?"]) else "‚ùå Nem",
                    axis=1
                )
                
                # --- R√©szletez≈ë t√°bla ---
                df_details = pd.merge(
                    df_nav,   # NAV oszlopok _nav
                    df_gpt,   # GPT oszlopok _ai
                    how="right",
                    left_on="sz√°mlasorsz√°m_nav",
                    right_on="Sz√°mlasz√°m_ai"
                )
                
                # Sz√°mlaszint≈± ellen≈ërz√©sek visszacsatol√°sa
                df_details = pd.merge(
                    df_details,
                    df_check[["Sz√°mlasz√°m_ai", "Brutt√≥ egyezik?", "Nett√≥ egyezik?", "√ÅFA egyezik?", "Minden egyezik?"]],
                    how="left",
                    on="Sz√°mlasz√°m_ai"
                )

                # Ment√©s session_state-be
                st.session_state.df_merged_nav = df_details

                # Statisztika sz√°mlasz√°m szinten
                total = len(df_check)
                matched_all = (df_check["Minden egyezik?"] == "‚úÖ Igen").sum()
                match_rate = round(100 * matched_all / total, 2)
    
                st.session_state.stats_nav = {
                    "√ñsszes sz√°mla": total,
                    "Minden egyez√©s": matched_all,
                    "Teljes egyez√©si ar√°ny (%)": match_rate
                }

                st.success("‚úÖ NAV f√°jllal val√≥ √∂sszef≈±z√©s √©s ellen≈ërz√©s k√©sz!")

            except Exception as e:
                st.error(f"V√°ratlan hiba t√∂rt√©nt a NAV √∂sszef≈±z√©s sor√°n: {e}")

    if "df_merged_nav" in st.session_state:
        st.write("üìÑ **√ñsszef≈±z√∂tt √©s ellen≈ërz√∂tt t√°bl√°zat ‚Äì NAV (t√©telszinten):**")
        st.dataframe(st.session_state.df_merged_nav)

        # Excel let√∂lt√©s
        buffer = BytesIO()
        with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
            st.session_state.df_merged_nav.to_excel(writer, sheet_name='NAV r√©szletek', index=False)

        st.download_button(
            label="üì• Let√∂lt√©s Excel (NAV r√©szletek)",
            data=buffer,
            file_name="merged_nav.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )

        st.markdown("### üìä Statisztika ‚Äì NAV √∂sszehasonl√≠t√°s")
        for k, v in st.session_state.stats_nav.items():
            st.write(f"**{k}:** {v}")


st.subheader("üìé Kinyert adatok √∂sszef≈±z√©se: Karton")

if st.button("üîó √ñsszef≈±z√©s a Kartonnal"):
    try:
        # GPT sz√°mlasz√°mok
        invoice_numbers = st.session_state.df_extracted["Sz√°mlasz√°m"].astype(str).unique()

        # Karton t√°bla el≈ëk√©sz√≠t√©se
        df_karton = st.session_state.df_karton.copy()
        df_karton.columns = [str(c).strip() for c in df_karton.columns]

        # Sz≈±r√©s: minden olyan sor kell, ahol b√°rmelyik oszlopban szerepel a sz√°mlasz√°m
        mask = df_karton.apply(lambda row: row.astype(str).isin(invoice_numbers).any(), axis=1)
        df_filtered_karton = df_karton[mask].copy()

        # Ha van "Bizonylat" vagy "Sz√°mlasz√°m" oszlop, rendezz√ºk arra
        for possible_col in ["Bizonylat", "Sz√°mlasz√°m", "sz√°mlasorsz√°m"]:
            if possible_col in df_filtered_karton.columns:
                df_filtered_karton = df_filtered_karton.sort_values(by=possible_col)
                break

        st.session_state.df_filtered_karton = df_filtered_karton

        # Statisztika: h√°ny GPT sz√°mlasz√°mhoz tal√°ltunk sorokat
        matched_karton = df_filtered_karton.apply(
            lambda row: any(str(val) in invoice_numbers for val in row.values), axis=1
        ).sum()
        total_karton = len(invoice_numbers)

        st.session_state.stats_karton = {
            "√ñsszes sz√°mla (GPT)": total_karton,
            "Kartonban megtal√°lt sorok": matched_karton,
        }

        st.success("‚úÖ Karton keres√©s √©s sz≈±r√©s k√©sz!")

    except Exception as e:
        st.error(f"‚ùå Hiba t√∂rt√©nt a Karton keres√©s sor√°n: {e}")

if "df_filtered_karton" in st.session_state:
    st.write("üìÑ **Sz≈±rt t√°bl√°zat ‚Äì Karton (csak relev√°ns sorok):**")
    st.dataframe(st.session_state.df_filtered_karton)

    # Excel let√∂lt√©s el≈ëk√©sz√≠t√©s
    buffer = BytesIO()
    with pd.ExcelWriter(buffer, engine="xlsxwriter") as writer:
        st.session_state.df_filtered_karton.to_excel(writer, sheet_name="Karton sz≈±rt", index=False)

    st.download_button(
        label="üì• Let√∂lt√©s Excel (Karton)",
        data=buffer,
        file_name="filtered_karton.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    )

    st.markdown("### üìä Statisztika ‚Äì Karton keres√©s")
    for k, v in st.session_state.stats_karton.items():
        st.write(f"**{k}:** {v}")






local_test = r"""

os.listdir('./')


import tomllib

secrets_path = r"C:\Users\abele\.streamlit\secrets.toml"

with open(secrets_path, "rb") as f:
    secrets = tomllib.load(f)

openai.api_key = secrets["OPENAI_API_KEY"]

MODEL = "gpt-4o"  # cheapest useful model
PDF_FILE = "9601013661.pdf"  # <-- replace with your own file path


with open(PDF_FILE, "rb") as f:
    text = extract_text_from_pdf(f)


# 2) GPT extraction
rows, tokens_used = extract_data_with_gpt(PDF_FILE, text, MODEL)


# 3) Create DataFrame
df = pd.DataFrame(rows, columns=[
    "F√°jl", "Elad√≥", "Vev≈ë", "Sz√°mlasz√°m", "Sz√°mla d√°tum",
    "Brutt√≥ √°r", "Nett√≥ √°r", "√ÅFA", "P√©nznem", "√Årfolyam"
])





"""



local_test = r"""
df_extracted = pd.read_csv('extract_data.csv')
df_minta = pd.read_excel('Mintav√©tel_k√∂lts√©gek_Sonneveld √©s ellen√Ørz√©s.xlsx', sheet_name='Mintav√©tel', skiprows = range(1, 9))
df_minta.columns = list(df_minta.iloc[0])
df_minta = df_minta.iloc[1:]
df_minta["Bizonylatsz√°m"] = df_minta["Bizonylatsz√°m"].astype(str)
df_karton = pd.read_excel('K√∂nyvel√©si karton 2024_Sonneveld Kft.xlsx', sheet_name='Munka1')

df_temp = pd.merge(df_minta, df_extracted, how='outer', left_on='Bizonylatsz√°m', right_on='Sz√°mlasz√°m')
nr_of_columns = len(df_temp.columns)

df_temp = pd.merge(df_temp, df_karton, how='left', left_on='Bizonylatsz√°m', right_on='Bizonylat')

column_to_compare = 'Bizonylatsz√°m'
columns_to_delete = df_temp.columns[:nr_of_columns]
df_merged = replace_successive_duplicates(df_temp, column_to_compare, columns_to_delete)

"""
